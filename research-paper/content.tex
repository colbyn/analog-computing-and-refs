\section{Prelude}

\begin{equation}
\begin{split}
    1\;\text{trillion} &= 1,000,000,000,000 = 1\,\mathrm{x}\,10^{12}\\
    1\;\text{human} &\approx 40\;\text{trillion}\;\mathrm{cells}\\
    1\;\text{cell} &\approx 100\;\text{trillion}\;\mathrm{atoms}
\end{split}
\end{equation}

If we needed to compute one instruction for each atom, for each cell, in the human body. Then we will require $4 \mathrm{x} 10^{27}$ instructions.

To imagine this in terms of time. Consider first, simply the latency overhead incurred when your processing hardware and main memory is physically separated from each other, and therefore, each instruction must first read some datum from main memory. Lets say this overhead is $100\mathrm{ns}$ per instruction. Then latency overhead alone will amount to $4\;\mathrm{x}\;10^{20}$ seconds, or in other words, latency overhead alone will total $12,683,916,800,000$ years per evolution.

But alas, this is a ludicrous exercise for a multitude of reasons. Because, for instance, what is meant by one instruction per atom? For each evolution, can the state of each atom be computed without considering neighboring interactions? Furthermore, can modern computers fundamentally handle such workloads?





\section{Continuous Computing}
\section{Von-Neumann Computer Architecture}



