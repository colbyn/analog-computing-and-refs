\subsection{A Future of Medicine, by Dr. Bernard}

% Dr. Bernard, A Future Of Medicine. Heme Review, \url{https://youtu.be/iVt5BpoTHYg}.

In his video essay, A Future of Medicine, Dr. Bernard postulates on a world that views our current system of evidence based medicine, as we see European plague doctors. Because in our modern system of evidence based medicine, we extrapolate population data to a given person, for the purpose of predicting outcome. Which you may ask, what is the problem with such? 

In his talk, he explain two outstanding issues. The first is about a documented ethical dilemma written by the New York Times, called ``New Drugs Stir Debate on Rules of Clinical Trials''. It's about two cousins who happen to develop the same type of cancer. Ultimately both got involved in a clinical trial where one was given the preexisting standard of treatment, and the other, given the new experimental treatment. The issue here is, mortality rate for the preexisting standard of treatment is so grim that one was effectively condemned to death. As his mother said, ``What gives them the right to play God?''\textsuperscript{(Harmon)}

But more relevant to my chosen topic. Efficacy is based on generalizations of the applicable demographic, which doesn’t necessarily accommodate differences between yourself and the general population therefrom. Furthermore, as Dr. Bernard points out, if a trial was done on a bunch of men, clinical experience suggests that there will be differences for women taking the same medication.\textsuperscript{(Bernard)}

Ultimately, what Dr. Bernard is aiming for is a -more ideal- hypothetical replacement to our current standard of evidence based medicine that is founded on computer modeling -technology that, as he points out, will probably not be available in our lifetime.


\subsection{Codebreaker: A deeply personal quest made Matthew Might a leader in precision medicine and brought him to UAB, by Cary Estes}

% Estes, Cary. ``Codebreaker: A deeply personal quest made Matthew Might a leader in precision medicine and brought him to UAB''. \url{https://www.uab.edu/medicine/magazine/178-codebreaker-a-deeply-personal-quest-made-matthew-might-a-leader-in-precision-medicine-and-brought-him-to-uab}.

This is about a very interesting story that isn't very well documented. But perhaps this source may be considered credible since it's by the University of Alabama at Birmingham.

Essentially, it's about a child who was born with a rare genetic disorder that had no known treatments. Why this is relevant to my talk is because this resembles what we may perhaps consider to be the forerunner to Dr. Bernard's hypothetical replacement to our current standard of evidence based medicine that is founded on computer modeling.

In this case, the father of this child, Matt Might, used computation to discover pathways (presumably using open biological databases) that work around a malformed protein. As he put it, in essence, if red increases green, and green decreases blue, then we can decrease blue by increasing red. In this case, he used the Kappa programming language and constructed a model based on a series of constraints that the computer must satisfy. It's regarded as an old form of machine learning, and unlike modern machine learning, the solution isn’t ‘inferred’. As in, ‘how’ it arrives at a given answer will be known.

Now, he’s a Professor, and Department of Medicine Director, at Hugh Kaul Precision Medicine Institute (and he doesn't even have a medical degree, it's in computer science).

\subsection{Analog synthetic biology by Rahul Sarpeshkar}

% Sarpeshkar, R. “Analog synthetic biology.” Philosophical transactions. Series A, Mathematical, physical, and engineering sciences vol. 372,2012 20130110. 24 Feb. 2014, doi:10.1098/rsta.2013.0110

While the above source is an instance of treatment that is perhaps a subset of Dr. Bernard's hypothetical system, this source pertains to the actual computing side. 

The motivations underlying my topic began from a TedX talk by a man called Rahul Sarpeshkar (Professor of Engineering, Thomas E. Kurtz Professor, Professor of Microbiology \& Immunology, Professor of Physics, Professor of Molecular \& Systems Biology). In his talk, Sarpeshkar discussed the prospects of analog computing for solving differential equations in a manner that far outpaces the capabilities of digital computers, and furthermore, he claimed that we could viably simulate an entire person if we built an analog computer that spanned the size of the given auditorium.

This is due, not to processing performance, but as Sarpeshkar argues, in information theory. That is, digital computation in contrast, while built upon analog mediums, forgoes most of the real estate therein for a limited set of gates defined in terms of a mere bit, in a single multi-bit analog channel. Conversely, in allowing full utilization of the unclaimed and unexploited real estate therefrom, in further expanding our conception of computation to more than mere logic, new applications may potentially become commercially viable. Such as perhaps, simulating the molecular interactions within cells, to tissues, to entire organ systems, to perhaps, entire persons.

Or rather, as Sarpeshkar summarized:

\begin{quotation}
    ``[at] low informational precision, logic basis functions simply cannot compete with the richer basis functions of analog computation that can process all the bits at once in parallel and just automatically solve the task, e.g. by using Kirchoff’s current law for addition or chemical binding for multiplication.''\textsuperscript{(Sarpeshkar)}
\end{quotation}


The central issue plaguing analog computers is that of precision. Analog computations are typically bounded to just three or four bits of precision. Yet, advocates of analog computers argue that many problems do not exceed such limitations, and therefore permits implementation in an analog environment that affords a greater degree of optimizations that aren't possible on digital computer architectures. 

In this context, Sarpeshkar argues that the fundamental limitations of analog computing, notably, noisy signals, is ideal for simulating stochastic process. Because simulating randomness on digital hardware imposes synchronization constraints that significantly impacts performance on such systems. Whereas on analog architectures, Sarpeshkar argues, you essentially get such for free.


Furthermore, the author postulates that there exists a deep connection between electrons and chemistry, in the sense that we can define an isomorphism (my own words) between the chemistry that manifests real life, and modeling the very same systems `electronically'.  As Sarpeshkar wrote,
\begin{quotation}
    ``There are striking similarities between chemical-reaction dynamics (figure 3a) and electronic current flow in the subthreshold regime of transistor operation (figure 3b): electron concentration at the source is analogous to reactant concentration; electron concentration at the drain is analogous to product concentration; forward and reverse current flows in the transistor are analogous to forward and reverse reaction rates in a chemical reaction; the forward and reverse currents in a transistor are exponential in voltage differences at its terminals analogous to reaction rates being exponential in the free-energy differences in a chemical reaction; increases in gate voltage lower energy barriers in a transistor increasing current flow analogous to the effects of enzymes or catalysts in chemical reactions that increase reaction rates; and the stochastics of the Poisson shot noise in subthreshold transistors are analogous to the stochastics of molecular shot noise in reactions. [...] The logarithmic dependence of the electrochemical potential in chemical concentration or of current enables one to map log-domain analog transistor circuit motifs in electronics to log-domain analog molecular circuit motifs in cells and vice versa.''\textsuperscript{(Sarpeshkar)}
\end{quotation}





