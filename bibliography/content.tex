% 1. Author name(s), credentials, and background (briefly).
% 2. Type of source (report, study, peer-reviewed journal article, website, book, podcast episode, blog post, newspaper article, museum exhibition, etc.) and the main topic it addresses.
% 3. Summary of the source’s methodology: authors use primary research (empirical; qualitative or quantitative);  or, authors refer to secondary research and offer a meta-analysis; or, authors write an interpretive essay or article in which they draw on other authors and their ideas or concepts. 
% 4. The main thesis and/or the conclusions most pertinent to your research questions or project goals. Avoid quotations--paraphrase instead, providing correct in-text parenthetical citations. Use author tags (such as “According to X” or “As X suggests”) in every sentence of your summary to attribute these details to the author(s) of the source. 
% 5. Any limitations or biases in the source; consider the set-up of the primary research, or the study sample, or the scope of the research. Often, researchers will address some limitations themselves in their article. 
% 6. Evaluation of how you might use the source in your upcoming essays. Use first-person meta commentary (such as “I intend” or “I plan”) in every sentence of your evaluation to attribute these details to yourself, and not the author(s) of your source.


% 3. Summary of the source’s methodology: authors use primary research (empirical; qualitative or quantitative);  or, authors refer to secondary research and offer a meta-analysis; or, authors write an interpretive essay or article in which they draw on other authors and their ideas or concepts. 

\section*{Authors Note}

I've decided to change my topic of interest. For the reasons given below (will probably be seen in the resulting paper).

In his video essay, A Future of Medicine, Dr. Bernard postulates on a world that views our current system of evidence based medicine, as we see European plague doctors. Because in our modern system of evidence based medicine, we extrapolate population data to a given person, for the purpose of predicting outcome. Which you may ask, what is the problem with such? 

To explain, from a practical standpoint, consider this. What if, instead of running a trial on thousands of disperse persons. What if you were simply cloned, hundreds of thousands of times, and therein, we performed hundreds of thousands of trials, forming a stochastic model that likewise predicts outcome for some given treatment?

As Dr. Bernard points out, if a trial was done on a bunch of men, there have been documented cases where women taking the very same medication experienced different reactions. Nowadays, trials are required to include notable age, gender, and race/ethnicity demographics. But as Kravitz pointed out, ``[this] may do nothing but ensure that the estimates for any one subgroup are unreliable due to small numbers''. In the very same paper, Kravitz coined this phenomena, the ``Heterogeneity of Treatment Effects''. 

Furthermore, Dr. Bernard likewise points out that, if this were possible, we could likewise extend this to drug development. Perhaps as I'd imagine, in a manner that may be inconceivable to us, as I will explain.

But you may ask, how exactly do we run hundreds of thousands of trials of my identical clones? We run computer simulations, as Dr. Bernard proposed. Although what Dr. Bernard proposed, was essentially a thought experiment, explicitly regarding this as something that probably won't be viable in our lifetime. Conversely, the following paper will say otherwise. 

This all began from a TedX talk by a man called Rahul Sarpeshkar (Professor of Engineering, Thomas E. Kurtz Professor, Professor of Microbiology \& Immunology, Professor of Physics, Professor of Molecular \& Systems Biology). In his talk, Sarpeshkar discussed the prospects of analog computing for solving differential equations in a manner that far outpaces the capabilities of digital computers, and furthermore, he claimed that we could viably simulate an entire person if we built an analog computer that spanned the size of the given auditorium.

Because, as he pointed out, ``Digital computation is a subset of analog computation that only operates at its saturated high or low extremes. Therefore, by allowing operation over the whole range of signal levels and by allowing exploitation of all the basis functions of biochemistry and biophysics, not just logic, horizons for computation are expanded. Digital computation will continue to be important for decision-making, signal restoration, communication and sequential operation.''

This is because, for instance, \begin{quotation}
    ``[the] fundamental reason for the analog–digital crossover does not lie in issues having to do with parameters and numbers. It really lies in information theory: information is coded across many 1-bit-precise interacting computational channels in the digital approach but on 1 multi-bit computational channel in the analog approach [14]. At low informational precision, logic basis functions simply cannot compete with the richer basis functions of analog computation that can process all the bits at once in parallel and just automatically solve the task, e.g. by using Kirchoff’s current law for addition or chemical binding for multiplication.''
\end{quotation}

Furthermore, to further expand upon the premise outlined in A Future of Medicine. I don't think this will be the full story to the totality of medicine. Because, simulations simply do one thing, they simulate, and therein, we build a probable stochastic model from such. Put simply, it's a mechanism that may be used to answer yes or no questions, but of course, what precedes treatment to some illness is the detection of such. 

In his book called ``The Body: A Guide for Occupants'', Bill Bryson regarded cancer and other such diseases as ``system failures'', and therein, he remarked, part of the problem is that our nervous system, paradoxically, may not register the early formation of cancer and other such events (given it's severity). But, perhaps we can go even further than the mere detection of cancer.

What precedes treatment is detection, and when this is continuous, we may regard such as monitoring, and this is where we enter into the domain of molecular programming. Can we run computational devices within cells? Yes. Furthermore, as Sarpeshkar noted in his paper, Analog synthetic biology, electronics (simulation) and chemistry (e.g. biochemical computation) are deeply linked. As Sarpeshkar wrote,
\begin{quotation}
    ``There are striking similarities between chemical-reaction dynamics (figure 3a) and electronic current flow in the subthreshold regime of transistor operation (figure 3b): electron concentration at the source is analogous to reactant concentration; electron concentration at the drain is analogous to product concentration; forward and reverse current flows in the transistor are analogous to forward and reverse reaction rates in a chemical reaction; the forward and reverse currents in a transistor are exponential in voltage differences at its terminals analogous to reaction rates being exponential in the free-energy differences in a chemical reaction; increases in gate voltage lower energy barriers in a transistor increasing current flow analogous to the effects of enzymes or catalysts in chemical reactions that increase reaction rates; and the stochastics of the Poisson shot noise in subthreshold transistors are analogous to the stochastics of molecular shot noise in reactions. [...] The logarithmic dependence of the electrochemical potential in chemical concentration or of current enables one to map log-domain analog transistor circuit motifs in electronics to log-domain analog molecular circuit motifs in cells and vice versa.''
\end{quotation}

Which he further extended and then summarized to, \begin{quotation}
    ``the cytomorphic [cell derived computation] mapping between electronics and chemistry outlined in [...] enables one to map from electronic circuits to DNA–protein circuits and vice versa. [...] The other direction [...] is useful for the design and simulation of synthetic biological circuits or the ultrafast stochastic simulation of large-scale systems-biology circuits with supercomputing chips.''
\end{quotation}

Therefore, we have an isomorphism between simulation on electronic infrastructure and biochemical reactions.

Which I argue, has far reaching implications. Because this reduces modeling to programming, and from programming, abstraction. Why does this matter? When we abstract, we may sometimes reduce `complex' things into simpler, more conceptually `discrete' things. Which therein, due to this simpler nature, may further permit others to effectively build upon such, and therein, may create something more sophisticated, perhaps even, greater than the sum of it's components.

That is, a system where discrete units build upon other discrete units and therein produce more complex non-discrete units. This system permits for abstraction, so complex non-discrete units may be abstracted into simple discrete units. Thereafter this process of production and abstraction enables further production and abstraction and so forth. Each iteration or generation may be considered to be more sophisticated than prior generations, given that each generation is a product of prior generations… From this analogy, you can imagine these bottom-up and cumulative processes will eventually give rise to very sophisticated products, and perhaps one day, akin to how emergence gives rise to the complexity found in nature.

From personal experience, my \url{https://imager.io} project wouldn’t be possible without the various open source components it’s built upon. Simply because my time is finite, and especially because lower-level encoding details are just \textbf{too complicated for me to understand and implement on my own}. I am nevertheless able to compose such components into a larger and more sophisticated end product, from the preexisting output of resources and information from the global open source, software community. Overall added value that may be considered to be greater than the sum of its components, and therefore emergent in a manner of speaking. In an old English paper I likened the open source community as ``the printing press of computable knowledge'', and perhaps even more significant than the advent of the printing press itself, because as the industrial revolution introduced a force multiplier of human muscle, so too does abstraction introduce a force multiplier of the human mind.

Because, as I've written, while a book may describe a life’s work in mathematics and applications therein, the medium is itself rather passive. A book may describe a life’s work in applied mathematics, yet a mind is required to manifest its application. Whereas, imagine a medium where the most knowledgeable of experts can record their understanding of a given domain as functions that map problems to solutions, in a manner that can be utilized by any layperson, and thereafter this record can be reapplied, reused, and so forth, forever thereafter.

The field of synthetic biology is one that attempts to unify engineering and biology. For this intersection, we have a foundation that reduces biology to software. With a proper software ecosystem in place, these bottom-up, cumulative processes may give rise to very sophisticated products, and perhaps one day, akin to the emergent phenomena seen in nature itself. Because, such an ecosystem is akin to the aforementioned ``force multiplier of the human mind''.

But perhaps too, just because software is typically easier to experiment and iterate on. You don't need a lab with specialized knowhow for the equipment, but in this case, perhaps, access to some cloud based compute infrastructure that affords easy and cheap access to the more specialized analog computing hardware. (If I happen to get into this, I'd perhaps call it SubSystems, given my \url{sub.systems} GTLD.)

What may concern us now, the implementation of analog computers for such workloads, but, as many have pointed out. When we speak of digital vs. analog. What we are really discussing is, computation in terms of discrete vs. continuous analogs. 

Digital computation is discreet, and proceeds in terms of discrete steps. All representations in digital form, are mere, and meaningless symbols. Computer arithmetic for instance, is simply the manipulation of such symbols is manner that implements such operations.

Sarpeshkar described the output of an analog computer as a ``1 multi-bit'' channel, can this be said to be a function? As is, an analog computer, is presumably initialized VIA a `discrete' input from a digital computer. Therefore, perhaps analog computers could be modeled as a Comonad from Haskell. That is, in Haskell syntax, akin to the following relation, $\forall\;a\;b.\;a\;\to\;m\;b$. Perhaps then, translating this to digital form is simply mapping $m\;a \to a$, therein, loosing context that can only be represented in an analog computer. Furthermore, if we constrained this to $\forall\;a.\;a\;\to\;m\;a$, then subsequently, all further computation in this machine will proceed as a series of natural transformations of the form $\forall\;a.\;f\;a \to g\;a$, since $a$ is never transformed, this should ensure continuity of all values of $x$ in $a$. But overall, in analog computation, perhaps it'd suffice to say that an analog model is parameterized in terms of time and space, and is therefore limited to continuous functions that satisfy the domain. 

Digital computers manipulate symbols, and can assign any arbitrary meaning to such. Whereas analog computers must maintain continuity, and therefore, must be defined as a series of transformations. The transformations may be a program, where the transformations themselves are built upon some more primitive set of gates. 

We can imagine these transformations as mappings of some $a\to{a}$, and where function application is an application between functions.

Each gate may be defined as some $\forall{a}.\;\mathrm{Constraint}\;a\Rightarrow\;a\to{a}$, where $\mathrm{Constraint}\;a$ represents some constraint that the composition of these gates must collectively satisfy. This provides some level of formal verification for our software. Given that, for instance, debugging such may be extremely difficult, or perhaps even, impossible. Furthermore, perhaps we can regard the unification of all of such constraints as the set representing the domain of our program, which we will regard as $S$. But, what about functions that transform the domain? For instance, what if some function $f$ introduces an asymptote, and another function $g$ cancels it out, and therefore the composition should likewise remove the constraint for subsequent functions. Likewise, addition should shift all the values within the domain by a given amount, including e.g. asymptotes. 

\dots

% Furthermore, what if, we wished to implement something that splits it's input into two identical channels, perhaps using tuples, akin to $\forall{a}.\;{a}\to{f}\;(a, a)$. 


\section*{Bibliography}

\subsection*{A Future of Medicine}
Dr. Bernard, A Future Of Medicine. Heme Review, \url{https://youtu.be/iVt5BpoTHYg}.
\begin{enumerate}
    \item Caravelli, Francesco, and Juan Carbajal.
    \item Peer-reviewed journal article.
    \item The author writes an interpretive video essay in which he draws upon prior research and events.
    \item In this essay, Dr. Bernard explains how our current medial system works and some more ideal hypothetical system, respectively. What is important, and the thesis of his essay, are the flaws of our current medical system (which he explains in exquisite detail). Overall, what is interesting -I think- is how our modern evidence based system isn’t as ideal as I once thought, and this is due to it’s dependence on population data, so such may not be directly applicable to a given person. For example, if a trial was done on a bunch of men, the speaker said clinical experience suggests that there will be differences for women taking the same medication. Whereas, his proposed hypothetical system disregards the need for such models VIA personalized simulations of a given individual, and I suppose all their biological processes. I.e. if this technology was available, there would be no need to study the efficacy VIA population models, because with this hypothetical technology, such simulations would be far more accurate, would be far more personalized to the biochemistry of the given individual, and not from generalizations of population data. Furthermore, if possible, he said, such could likewise -hypothetically speaking- be extended to drug development VIA optimization algorithms. \textsuperscript{(Bernard)}
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}


\subsection*{Memristors for the Curious Outsiders}
Caravelli, Francesco, and Juan Carbajal. “Memristors for the Curious Outsiders.” Technologies 6.4 (2018): 118. Crossref. Web.
\begin{enumerate}
    \item Caravelli, Francesco, and Juan Carbajal.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item An overview is given on a relatively new technology called ``Memristors'', and it's potential applications.
    
    What memristors bring to the table (among other things) is more efficient computation for certain application specific workloads. To explain, consider the most performant offering from the Google cloud A2 Accelerator-Optimized tier, the ``Nvidia A100 tensor core GPU'', which can perform $3.12$ trillion (four byte) operations per watt. While in contrast, the human brain can perform about $83.\bar{3}$ trillion operations per watt. So therefore, while akin to an apples to oranges comparison, there may be room for improvement for low precision workloads. In this case, memristors may be ideal for implementing artificial neural networks.
        \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}



\subsection*{Neural Networks and Analog Computation Beyond the Turing Limit}
Hava T. Siegelmann. 1999. Neural networks and analog computation: beyond the Turing limit. Birkhauser Boston Inc.
\begin{enumerate}
    \item Hava T. Siegelmann.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item In this 192 page document, the author discusses analog computation in the context of artificial neural networks. (Which is the sort of technology that can build upon the aforementioned ``Memristors''). The author begins with a discussion on the shortcomings of the predominate von neumann computer architecture, which is a model where memory and computation are discrete and physically separate entities. 
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

\subsection*{Quantum Computing with Analog Circuits: Hilbert Space Computing}
Kish, Laszlo. (2003). Quantum Computing with Analog Circuits: Hilbert Space Computing. Proceedings of SPIE - The International Society for Optical Engineering. 5055. 10.1117/12.497438. 
\begin{enumerate}
    \item Kish, Laszlo.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item The author postulates that Quantum Computing may be implemented in a commercially viable manner as a classical analog computer. 
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

\subsection*{A Review of Analog Computing}
MacLennan, Bruce (2007). A review of analog computing. Technical Report CS-07-601, Department of Electrical Engineering \& Computer Science. University of Tennessee, Knoxville
\begin{enumerate}
    \item MacLennan, Bruce.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item The author gives an overview of analog computing.
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

\subsection*{Computability of analog networks}
John V. Tucker and Jeffery I. Zucker. 2007. Computability of analog networks. Theor. Comput. Sci. 371, 1–2 (February, 2007), 115–146. DOI:https://doi.org/10.1016/j.tcs.2006.10.018
\begin{enumerate}
    \item John V. Tucker and Jeffery I. Zucker.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item The authors discusses an implementation of analog computing with respect to a global continuous clock.
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

\subsection*{It’s an analog world}
Sauro, H., Kim, K. It's an analog world. Nature 497, 572–573 (2013). https://doi.org/10.1038/nature12246
\begin{enumerate}
    \item John V. Tucker and Jeffery I. Zucker.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item The authors discuss implementing functional circuits in living systems based on an analog model.
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}


\subsection*{Analog synthetic biology}
Sarpeshkar, R. “Analog synthetic biology.” Philosophical transactions. Series A, Mathematical, physical, and engineering sciences vol. 372,2012 20130110. 24 Feb. 2014, doi:10.1098/rsta.2013.0110
\begin{enumerate}
    \item John V. Tucker and Jeffery I. Zucker.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item The author discusses analog computing in the context of systems biology, both in terms of encoding computation in biological systems, and from the perspective of simulating these very same systems electronically. Furthermore, the author postulates that there exists a deep connection between electrons and chemistry, in the sense that we can define an isomorphism (my own words) between the chemistry that manifests real life, and modeling the very same systems `electronically'.  
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

\subsection*{Evidence-Based Medicine, Heterogeneity of Treatment Effects, and the Trouble with Averages}
Kravitz, Richard L et al. “Evidence-based medicine, heterogeneity of treatment effects, and the trouble with averages.” The Milbank quarterly vol. 82,4 (2004): 661-87. doi:10.1111/j.0887-378X.2004.00327.x
\begin{enumerate}
    \item Kravitz, Richard.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item The authors discuss the difficulties of our modern evidence based system of medicine. This is, we rely on population models to predict outcome for a given patent. Or put another way, we make generalizations, and so therefore, we may ask, do these generalizations always hold up? In their paper, the authors essentially say no, due to a phenomena the authors call, the ``Heterogeneity of Treatment Effects ''. The authors later discuss the implications for relevant parties. 
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

\subsection*{Computation Beyond the Turing Limit}
Siegelmann, H T. “Computation beyond the turing limit.” Science (New York, N.Y.) vol. 268,5210 (1995): 545-8. doi:10.1126/science.268.5210.545
\begin{enumerate}
    \item Siegelmann, Hava.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item This a short, and very difficult read. It's the kind of literature that I expect to one day understand, and therefore, make a personal note to review this at such a time. As is, note it's existence, in the chance that I happen to be working in the area, but I digress.
    
    In the paper, the author deposits a philosophical claim, that in humanities quest in both deciphering and mastering nature, has inevitably become a quest in building machines that can simulate such systems. In this paper, the author discusses simulating natural phenomena using a particular manifestation of analog computing, which is claimed to exceed the performance of todays computer architectures (presumably referring to the ``Von neumann computer architecture''). 
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}



\subsection*{Biological switches and clocks}
Tyson, John \& Albert, Reka \& Goldbeter, Albert \& Ruoff, Peter \& Sible, Jill. (2008). Biological switches and clocks. Journal of the Royal Society, Interface / the Royal Society. 5 Suppl 1. S1-8. 10.1098/rsif.2008.0179.focus. 
\begin{enumerate}
    \item Tyson, John \& Albert, Reka \& Goldbeter, Albert \& Ruoff, Peter \& Sible, Jill.
    \item Peer-reviewed journal article.
    \item Authors refer to secondary research and offer a meta-analysis.
    \item To my understanding, only $1.5\%$ of your DNA encodes for proteins, while about $5\%$ of your DNA are regulatory sequences.
    
    The authors begin with a remark on the similarities between the information processing facilities of cells and that of manmade computers, and that, just as a ``sophisticated theory of electronic circuitry'' made modern manmade computers possible, so too, will a similar model of biomolecular circuitry give way to exploiting the regulatory mechanisms of cellular systems. 

    Later in, the authors discuss recent advancements from mathematical modelers in explaining such natural phenomena.
    \item None given. 
    \item I plan to use such in my analysis of analog computing. 
\end{enumerate}

